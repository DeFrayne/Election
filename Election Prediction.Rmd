---
title: "Election Prediction"
author: "DeFrayne & Titus"
date: "October 30, 2020"
output: html_document
---

```{r}
#Load librarries
if (!require('caret')){
  install.packages('caret')
  require('caret')
}
if (!require('tidyverse')){
  install.packages('tidyverse')
  require('tidyverse')
}
if (!require('dplyr')){
  install.packages('dplyr')
  require('dplyr')
}
if (!require('tidyr')){
  install.packages('tidyr')
  require('tidyr')
}
if (!require('corrplot')){
  install.packages('corrplot')
  require('corrplot')
}
if (!require('readxl')){
  install.packages('readxl')
  require('readxl')
}
if (!require('doParallel')){
  install.packages('doParallel')
  require('doParallel')
}
if (!require('rstan')){
  install.packages('rstan')
  require('rstan')
}
if (!require('bayesplot')){
  install.packages('bayesplot')
  require('bayesplot')
}
if (!require('usdata')){
  install.packages('usdata')
  require('usdata')
}

```

```{r}
#Set up - only run first time

#Set Working Directory
#setwd("D:/Git/Election/Data")

#Download all data from sources
Popular_Data <- read.csv(url("https://raw.githubusercontent.com/TheEconomist/us-potus-model/master/data/abramowitz_data.csv"))
State_Data <- read.csv(url("https://raw.githubusercontent.com/TheEconomist/us-potus-model/master/data/potus_results_76_16.csv"))
Polls_2008 <- read.csv(url("https://raw.githubusercontent.com/TheEconomist/us-potus-model/master/data/all_polls_2008.csv"))
Polls_2012 <- read.csv(url("https://raw.githubusercontent.com/TheEconomist/us-potus-model/master/data/all_polls_2012.csv"))
Polls_2016 <- read.csv(url("https://raw.githubusercontent.com/TheEconomist/us-potus-model/e7a027bfbb5b3f177e159db3ab0a82d2d6ad379e/data/all_polls.csv"))
Polls_2020 <- read.csv(url("https://projects.fivethirtyeight.com/polls-page/president_polls.csv"))

#Save All Data Locally
write.csv(Popular_Data, "Time_For_Change_Data.csv", row.names=FALSE)
write.csv(State_Data, "State_Data.csv", row.names=FALSE)
write.csv(Polls_2008, "Polls_2008.csv", row.names=FALSE)
write.csv(Polls_2012, "Polls_2012.csv", row.names=FALSE)
write.csv(Polls_2016, "Polls_2016.csv", row.names=FALSE)
write.csv(Polls_2020, "Polls_2020.csv", row.names=FALSE)
```

```{r}
#Lizzy's Experiments
#Add a read-in from the direct data source in the above section
#setwd("D:/Git/Election/Data")
#Other_Data <- read_excel("WIID_06MAY2020.xlsx")


####################
# State Model Data #
####################
StateDataFull <- read.csv(url("https://raw.githubusercontent.com/DeFrayne/Election/master/Data/State%20Data/StateDataMaster.csv"))

###############
# STATE MODEL #
###############
#split into past and current data
PastStateData <- StateDataFull %>%
  filter(year != 2020)

StateData2020 <- StateDataFull %>%
  filter(year == 2020)

#Initiate Parallel Processing
cl <- makeCluster(detectCores()*0.5)
registerDoParallel(cl)

# Create the state regression model
Train_Control <- trainControl(method="cv", number=10)
State_Model <- train(incvote ~ inc1  + Change  + MedIncome,
                         data = PastStateData,
                         trControl = Train_Control,
                         #tuneGrid = Tune_Grid, for glmnet
                         method="glm")

#End cluster and resume sequential processing
stopCluster(cl)
registerDoSEQ()

# info on state model
summary(State_Model)

#######################
# NATIONAL MODEL Data #
#######################
NationalData <- read.csv(url("https://raw.githubusercontent.com/DeFrayne/Election/master/Data/National%20Data/NationalDataMaster.csv"))


##################
# NATIONAL MODEL #
##################
#split into past and current data
PastNationalData <- NationalData %>%
  filter(year != 2020)
NationalData2020 <- NationalData %>%
  filter(year == 2020)


#Initiate Parallel Processing
cl <- makeCluster(detectCores()*0.5)
registerDoParallel(cl)


#Create the National Model 
Train_Control <- trainControl(method="cv", number=2)
#Tune_Grid <- expand.grid('alpha'=c(.5, 1),'lambda' = seq(0.0001, .01, length.out = 10))
National_Model <- train(incvote ~ q2gdp + juneapp + inc1,
                               data = PastNatinalData,
                               trControl = Train_Control,
                               #tuneGrid = Tune_Grid, for glmnet
                               method="glm")

#End cluster and resume sequential processing
stopCluster(cl)
registerDoSEQ()

# info on National Model
summary(National_Model)


```

```{r}
##################
# NATIONAL MODEL #
##################



#Test the files to ensure we receive the same results of Abramowitz

#Set Working Directory
#setwd("D:/Git/Election/Data")

#Read in Time For Change Data
Time_For_Change_Data <- read.csv("Time_For_Change_Data.csv", stringsAsFactors=FALSE)

#Remove 2016 Data
Time_For_Change_Data_No_2016 <- Time_For_Change_Data %>%
  filter(year != "2016") %>%
  select(-label, -filter_.)

#Only 2016 data
Time_For_Change_Data_Only_2016 <- Time_For_Change_Data %>%
  filter(year == "2016") %>%
  select(-label, -filter_.)


#Cross-validation
#Train_Control <- trainControl(method="LOOCV")
Train_Control <- trainControl(method="cv", number=2)
Tune_Grid <- expand.grid('alpha'=c(.5, 1),'lambda' = seq(0.0001, .01, length.out = 10))

#Initiate Parallel Processing
cl <- makeCluster(detectCores()*0.5)
registerDoParallel(cl)


#Create the regression model for 2016 prediction
Time_For_Change_Model_2016 <- train(incvote ~ q2gdp + juneapp + inc1,
                               data = Time_For_Change_Data_No_2016,
                               trControl = Train_Control,
                               #tuneGrid = Tune_Grid, for glmnet
                               method="glm")

#Create the regression model for 2020 prediction (2020 data not included yet)
Time_For_Change_Model_2020 <- train(incvote ~ q2gdp + juneapp + inc1,
                               data = Time_For_Change_Data,
                               trControl = Train_Control,
                               #tuneGrid = Tune_Grid, for glmnet
                               method="glm")

#Check only 1976 to 2016 (leave this here - notice that none of the variables are significant when we check 1976
#to 2016); I think that's a good talking point re: shifting demographics
TFC_Start_1976 <- Time_For_Change_Data %>%
  filter(year >= "1976")
TFC_Start_1976_Model <- train(incvote ~ q2gdp + juneapp + inc1,
                               data = TFC_Start_1976,
                               trControl = Train_Control,
                               #tuneGrid = Tune_Grid, for glmnet
                               method="glm")

#End cluster and resume sequential processing
stopCluster(cl)
registerDoSEQ()

#Show summary statistics for logistic regression model for 2016 prediction 
summary(Time_For_Change_Model_2016)


#check accuracy of model 
predict(Time_For_Change_Model_2016, newdata=Time_For_Change_Data_Only_2016)
```


```{r}
#Exploratory data analysis
Time_For_Change_Correlations <- Time_For_Change_Data %>%
  select(q2gdp, juneapp, inc1)
corrplot(cor(Time_For_Change_Correlations), tl.cex = 0.5)

#Show model trend vs. real stats
plot(Time_For_Change_Data$year, Time_For_Change_Data$incvote, col="red", pch=20)
points(Time_For_Change_Data$year, predict(Time_For_Change_Model_2020), col="blue", pch=20)
plot(Time_For_Change_Data$incvote,predict(Time_For_Change_Model_2020))
```

```{r}
#Setup for the state-based model
#setwd("D:/Git/Election/Data")
State_Data <- read.csv("State_Data.csv", stringsAsFactors = FALSE)
Time_For_Change_Data <- read.csv("Time_For_Change_Data.csv", stringsAsFactors=FALSE)

#Add in which party is incumbent to calculate voter shares for the incumbent candidate
year <- seq(1976, 2020, by=4)
incumbent_party <- c("R", "D", "R", "R", "R", "D", "D", "R", "R", "D", "D", "R")
incumbent_data <- data.frame(year, incumbent_party)

#Merge national statistics and incumbent party in to State_Data
State_Data <- State_Data %>%
  left_join(Time_For_Change_Data %>% select(q2gdp, juneapp, inc1, year),
            by="year")

State_Data <- State_Data %>%
  left_join(incumbent_data, by="year")

#Recalculate voting proportion based on incumbency rather than party
State_Data <- State_Data %>%
  mutate(incvote = ifelse(incumbent_party == "D",
                         dem/(dem+rep),
                         rep/(dem+rep)))

#Remove 2016 Data
State_Data_No_2016 <- State_Data %>%
  filter(year != "2016") 



#Only 2016 data
State_Data_2016 <- State_Data %>%
  filter(year == "2016")


Train_Control <- trainControl(method="cv", number=10)

#Initiate Parallel Processing
cl <- makeCluster(detectCores()*0.5)
registerDoParallel(cl)

#Create the regression model for 2016 prediction; notice that juneapp does not correlate, so this will provide
#a good excuse for using polling for a prior and removing this variable from our state-based model later
State_Model_No_2016 <- train(incvote ~ q2gdp + juneapp + inc1,
                               data = State_Data_No_2016,
                               trControl = Train_Control,
                               #tuneGrid = Tune_Grid, for glmnet
                               method="glm")

#End cluster and resume sequential processing
stopCluster(cl)
registerDoSEQ()

```

```{r}
#Cleaning Polling Data
#setwd("D:/Git/Election/Data")

Polls_2016 <- read.csv("Polls_2016.csv", stringsAsFactors = FALSE)
Polls_2020 <- read.csv("Polls_2020.csv", stringsAsFactors = FALSE)

Polls_2016 <- Polls_2016 %>%
  select(!X, !johnson, !mcmullin, !question.iteration, !question.text) %>%
  mutate(state = ifelse(state=="--", "National", state))

#######################
# Polling Exploration #
#######################

```

```{r}
#Rstan implimentation of the Time_For_Change Model

#Set Working Directory
#setwd("D:/Git/Election/Data")

#Read in Time For Change Data
Time_For_Change_Data <- read.csv("Time_For_Change_Data.csv", stringsAsFactors=FALSE)

#Enable parallel processing in Stan
options(mc.cores = parallel::detectCores())

#Remove 2016 Data
Time_For_Change_Data_No_2016 <- Time_For_Change_Data %>%
  filter(year != "2016") %>%
  select(-label, -filter_.)

#Only 2016 data
Time_For_Change_Data_Only_2016 <- Time_For_Change_Data %>%
  filter(year == "2016") %>%
  select(-label, -filter_.)

#Reconfigure the model in Stan
y_incvote <- Time_For_Change_Data_No_2016$incvote
x_q2gdp <- Time_For_Change_Data_No_2016$q2gdp
x_juneapp <- Time_For_Change_Data_No_2016$juneapp
x_inc1 <- Time_For_Change_Data_No_2016$inc1
N <- length(Time_For_Change_Data_No_2016$year)

stan_data <- list(
  N = N,
  x1 = x_q2gdp,
  x2 = x_juneapp,
  x3 = x_inc1,
  y = y_incvote
)

write("//Stan model for simple linear regression

data {
  int < lower = 1 > N; //Sample Size
  vector[N] x1; //Quarter 2 GDP
  vector[N] x2; //Gallop's June Approval Rating
  vector[N] x3; //Incumbency Score (Dummy Variable)
  vector[N] y; //Supervisor
}

parameters {
  real alpha; //Intercept
  real beta_q2gdp; //Slope
  real beta_juneapp; //Slope
  real beta_inc1; //Slope
  real < lower = 0 > sigma; //Error Standard Deviation
}

model {
  y ~ normal(alpha + x1 * beta_q2gdp + x2 * beta_juneapp + x3 * beta_inc1 , sigma); //Likelihood
}

generated quantities {

} //The posterior predictive distribution",

"stan_model.stan"
)

stanc("stan_model.stan")
stan_model <- "stan_model.stan"

fit <- stan(file = stan_model, data = stan_data, warmup = 1000, iter = 10000, chains = 4, cores = detectCores()*0.5, thin = 1)

posterior <- extract(fit)
```

```{r}
#RStan Experiments
#setwd("D:/Data")
seaice <- read.csv(url("https://raw.githubusercontent.com/ourcodingclub/CC-Stan-intro/master/seaice.csv"), stringsAsFactors = FALSE)

#Enable parallel processing in Stan
options(mc.cores = parallel::detectCores())

head(seaice)

colnames(seaice) <- c("year", "extent_north", "extent_south")

plot(extent_north ~ year, data=seaice, pch=20)

linear_model_1 <- lm(extent_north ~ year, data=seaice)
summary(linear_model_1)

#Does changing year to a time series alter the model?
seaice <- seaice %>%
  mutate(year_number = year - (min(year)-1))
linear_model_2 <- lm(extent_north ~ x, data=seaice)
summary(linear_model_2)

abline(linear_model_1, col="red", lw=4, lty=8)

#Reconfigure the model in Stan
x <- I(seaice$year - 1978)
y <- seaice$extent_north
N <- length(seaice$year)
linear_model_3 <- lm(y ~ x)
summary(linear_model_3)

lm3_alpha <- summary(linear_model_3)$coeff[1]
lm3_beta <- summary(linear_model_3)$coeff[2]
lm3_sigma <- sigma(linear_model_3)

stan_data <- list(
  N = N,
  x = x,
  y = y
)

write("//Stan model for simple linear regression

data {
  int < lower = 1 > N; //Sample Size
  vector[N] x; //Predictor
  vector[N] y; //Supervisor
}

parameters {
  real alpha; //Intercept
  real beta; //Slope
  real < lower = 0 > sigma; //Error Standard Deviation
}

model {
  alpha ~ normal(10,0.1); //Prior - these are bad priors to illustrate their power
  beta ~ normal(1, 0.1); //Prior
  y ~ normal(alpha + x * beta , sigma); //Likelihood
}

generated quantities {

} //The posterior predictive distribution",

"stan_model1.stan"
)

stanc("stan_model1.stan")
stan_model1 <- "stan_model1.stan"

fit <- stan(file = stan_model1, data = stan_data, warmup = 500, iter = 1000, chains = 4, cores = detectCores()*0.5, thin = 1)

posterior <- extract(fit)

plot(y ~ x, pch=20)
for (i in 1:500) {
 abline(posterior$alpha[i], posterior$beta[i], col = "gray", lty = 1)
}
abline(linear_model_3, col = "red", lty = 2, lw = 3)
abline(mean(posterior$alpha), mean(posterior$beta), col=6, lw=2)

plot(posterior$alpha, type = "l")
plot(posterior$beta, type = "l")
plot(posterior$sigma, type = "l")

#This is what bad mixing looks like
fit_bad <- stan(stan_model1, data = stan_data, warmup = 25, iter = 50, chains = 4, cores = 2, thin = 1)
posterior_bad <- extract(fit_bad)
plot(posterior_bad$alpha, type = "l")
plot(posterior_bad$beta, type = "l")
plot(posterior_bad$sigma, type = "l")

#Individual parameter distributions
par(mfrow = c(1,3))
plot(density(posterior$alpha), main = "Alpha")
abline(v = lm3_alpha, col = 4, lty = 2)
plot(density(posterior$beta), main = "Beta")
abline(v = lm3_beta, col = 4, lty = 2)
plot(density(posterior$sigma), main = "Sigma")
abline(v = lm3_sigma, col = 4, lty = 2)

#Check mixing and distributions in stan
traceplot(fit)
stan_dens(fit)
stan_hist(fit)

#Parameter estimates, graphically
plot(fit, show_density = FALSE, ci_level = 0.5, outer_level = 0.95, fill_color = "salmon")


#Check this code for posterior predictive checks later
write("// Stan model for simple linear regression

data {
 int < lower = 1 > N; // Sample size
 vector[N] x; // Predictor
 vector[N] y; // Outcome
}

parameters {
 real alpha; // Intercept
 real beta; // Slope (regression coefficients)
 real < lower = 0 > sigma; // Error SD
}

model {
 y ~ normal(x * beta + alpha, sigma);
}

generated quantities {
 real y_rep[N];

 for (n in 1:N) {
 y_rep[n] = normal_rng(x[n] * beta + alpha, sigma);
 }

}",

"stan_model2_GQ.stan")

stan_model2_GQ <- "scripts/users/imyerssmith/CC-Stan-Part-1/stan_model2_GQ.stan"

fit3 <- stan(stan_model2_GQ, data = stan_data, iter = 1000, chains = 4, cores = 2, thin = 1)

y_rep <- as.matrix(fit3, pars = "y_rep")
dim(y_rep)

ppc_dens_overlay(y, y_rep[1:200, ])

ppc_stat(y = y, yrep = y_rep, stat = "mean")

ppc_scatter_avg(y = y, yrep = y_rep)

```