---
title: "Election Prediction"
author: "DeFrayne"
date: "October 30, 2020"
output: html_document
---

```{r}
#Load librarries
if (!require('caret')){
  install.packages('caret')
  require('caret')
}
if (!require('dplyr')){
  install.packages('dplyr')
  require('dplyr')
}
if (!require('tidyr')){
  install.packages('tidyr')
  require('tidyr')
}
if (!require('corrplot')){
  install.packages('corrplot')
  require('corrplot')
}
if (!require('readxl')){
  install.packages('readxl')
  require('readxl')
}
if (!require('doParallel')){
  install.packages('doParallel')
  require('doParallel')
}


```

```{r}
#Set up - only run first time

#Set Working Directory
setwd("D:/Git/Election/Data")

#Read in the Time For Change model data from The Economist github account
#Popular_Data <- read.csv(url("https://raw.githubusercontent.com/TheEconomist/us-potus-model/master/data/abramowitz_data.csv"))
#State_Data <- read.csv(url("https://raw.githubusercontent.com/TheEconomist/us-potus-model/master/data/potus_results_76_16.csv"))
#Polls_2008 <- read.csv(url("https://raw.githubusercontent.com/TheEconomist/us-potus-model/master/data/all_polls_2008.csv"))
#polls_2012 <- read.csv(url("https://raw.githubusercontent.com/TheEconomist/us-potus-model/master/data/all_polls_2012.csv"))
#Polls_2016 <- read.csv(url("https://raw.githubusercontent.com/TheEconomist/us-potus-model/e7a027bfbb5b3f177e159db3ab0a82d2d6ad379e/data/all_polls.csv"))
#Polls_2020 <- read_csv(url("https://projects.fivethirtyeight.com/polls-page/president_polls.csv"))

#Save Time For Change model data locally
#write.csv(Popular_Data, "Time_For_Change_Data.csv", row.names=FALSE)
#write.csv(State_Data, "State_Data", row.names=FALSE)
#write.csv(Polls_2008, "Polls_2008.csv", row.names=FALSE)
#write.csv(Polls_2012, "Polls_2012.csv", row.names=FALSE)
#write.csv(Polls_2016, "Polls_2016.csv", row.names=FALSE)
#write.csv(Polls_2020, "Polls_2020.csv", row.names=FALSE)
```

```{r}
#Lizzy's Experiments
#Add a read-in from the direct data source in the above section
#setwd("D:/Git/Election/Data")

Other_Data <- read_excel("WIID_06MAY2020.xlsx")

# Possible Variables to add to model
Unemp            <- c(3.7, 3.0, 3.9, 6.1, 5.1, 3.4, 5.6, 7.7, 7.5, 7.4, 5.4, 7.3, 5.2, 3.9, 5.5, 6.5, 7.8, 4.9)
Turnout          <- c(51.1, 61.6, 59.3, 62.8, 61.4, 60.7, 55.1, 53.6, 52.8, 53.3, 50.3, 55.2, 49.0, 50.5, 55.7, 57.1,                        53.8, 54.8)
Petitions        <- c(68265, 94086, 137701, 127543, 113218, 103085, 121883, 199152, 192230, 286440, 237752, 342238,                          1277403, 460916, 662796, 525786, 899162, 972151)
Denied           <- c(2887, 2163, 3935, 2277, 2309, 1962, 1837, 2799, 4370, 3373, 4304, 19293, 229842, 399670, 103339,                       121283, 65874, 86033)
Prop_Denied      <- Denied/Petitions
Pop_Growth_Rate  <- c(1.74, 1.49, 1.76, 1.60, 1.27, 0.93, 0.89, 0.92, 94, 0.94, 0.95, 0.96, 1.20, 1.14, 0.88, 0.96,                          0.79, 0.67)


# Import dataset for gini measure 
Other_Data_US <- Other_Data %>%
  filter(country == "United States") %>%
  filter(source == "National statistical authority") %>%
  select(year, gini_reported, ratio_top20bottom20, bottom40, top5) %>%
  filter(top5 != "NA") %>%
  filter(year == 1948|
         year == 1952|
         year == 1956|
         year == 1960|
         year == 1964|
         year == 1968|
         year == 1972|
         year == 1976|
         year == 1980|
         year == 1984|
         year == 1988|
         year == 1992|
         year == 1996|
         year == 2000|
         year == 2004|
         year == 2008|
         year == 2012|
         year == 2016) %>%
  group_by(year) %>%
  summarise(gini = mean(gini_reported), Ratio = mean(ratio_top20bottom20), 
            Lower40 = mean(bottom40), Upper5 = mean(top5))

# Add possible variables to Time_For_Change Dataset
Other_Data_US         <- as.data.frame(Other_Data_US)
Time_For_Change_Data  <- data.frame(Time_For_Change_Data, Unemp, Turnout, 
                              Petitions, Denied, Prop_Denied, Pop_Growth_Rate)
Time_For_Change_Data  <- merge(Time_For_Change_Data, Other_Data_US)

###################################################################################
# This whole section is just me playing with possible combinations, please ignore #
###################################################################################
# Create a model with all possible variables 
Full_Model <- train(incvote ~  q2gdp + juneapp + inc1 + Unemp + Turnout + Petitions + Denied + Prop_Denied + Pop_Growth_Rate + gini + Ratio + Lower40 + Upper5, 
                              data = Time_For_Change_Data_No_2016,
                               trControl = Train_Control,
                               method="glm")

summary(Full_Model)

# Full model minus turnout and ratio and Petitions and denied
Full_Model_1 <- train(incvote ~  q2gdp + juneapp + inc1 + Unemp + Prop_Denied + Pop_Growth_Rate + gini + Lower40 + Upper5, 
                              data = Time_For_Change_Data_No_2016,
                               trControl = Train_Control,
                               method="glm")
summary(Full_Model_1)


# Full model minus turnout and ratio and Petitions and denied and unemp
Full_Model_2 <- train(incvote ~  q2gdp + juneapp + inc1  + Prop_Denied + Pop_Growth_Rate + gini + Lower40 + Upper5, 
                              data = Time_For_Change_Data_No_2016,
                               trControl = Train_Control,
                               method="glm")
summary(Full_Model_2)


# Full model minus turnout and ratio and Petitions and denied and unemp and Population Growth Rate
Full_Model_3 <- train(incvote ~  q2gdp + juneapp + inc1  + Prop_Denied +  gini + Lower40 + Upper5, 
                              data = Time_For_Change_Data_No_2016,
                               trControl = Train_Control,
                               method="glm")
summary(Full_Model_3)


# Full model minus turnout and ratio and Petitions and denied and unemp and Population Growth Rate and Lower40
Full_Model_4 <- train(incvote ~  q2gdp + juneapp + inc1  + Prop_Denied +  gini  + Upper5, 
                              data = Time_For_Change_Data_No_2016,
                               trControl = Train_Control,
                               method="glm")
summary(Full_Model_4)


# Full model minus turnout and ratio and Petitions and denied and unemp and Population Growth Rate and Lower40 and gini
Full_Model_5 <- train(incvote ~  q2gdp + juneapp + inc1  + Prop_Denied   + Upper5, 
                              data = Time_For_Change_Data_No_2016,
                               trControl = Train_Control,
                               method="glm")
summary(Full_Model_5)

# Full model minus turnout and ratio and Petitions and denied and unemp and Population Growth Rate and Lower40 and giniand upper5 plus interaction terms
Full_Model_6 <- train(incvote ~  (q2gdp + juneapp + inc1  + Prop_Denied)^2, 
                              data = Time_For_Change_Data_No_2016,
                               trControl = Train_Control,
                               method="glm")
summary(Full_Model_6)

```

```{r}
#Test the files to ensure we receive the same results of Abramowitz

#Set Working Directory
setwd("D:/Git/Election/Data")

#Read in Time For Change Data
Time_For_Change_Data <- read.csv("Time_For_Change_Data.csv", stringsAsFactors=FALSE)

#Remove 2016 Data
Time_For_Change_Data_No_2016 <- Time_For_Change_Data %>%
  filter(year != "2016") %>%
  select(-label, -filter_.)

#Only 2016 data
Time_For_Change_Data_Only_2016 <- Time_For_Change_Data %>%
  filter(year == "2016") %>%
  select(-label, -filter_.)


#Cross-validation
#Train_Control <- trainControl(method="LOOCV")
Train_Control <- trainControl(method="cv", number=2)
Tune_Grid <- expand.grid('alpha'=c(.5, 1),'lambda' = seq(0.0001, .01, length.out = 10))

#Initiate Parallel Processing
cl <- makeCluster(detectCores()*0.5)
registerDoParallel(cl)

#Create the logistic regression model for 2016 prediction
Time_For_Change_Model_2016 <- train(incvote ~ q2gdp + juneapp + inc1,
                               data = Time_For_Change_Data_No_2016,
                               trControl = Train_Control,
                               tuneGrid = Tune_Grid,
                               method="glmnet")

#Create the logistic regression model for 2020 prediction (2020 data not included yet)
Time_For_Change_Model_2020 <- train(incvote ~ q2gdp + juneapp + inc1,
                               data = Time_For_Change_Data,
                               trControl = Train_Control,
                               tuneGrid = Tune_Grid,
                               method="glmnet")

#End cluster and resume sequential processing
stopCluster(cl)
registerDoSEQ()


#Show summary statistics for logistic regression model for 2016 prediction 
summary(Time_For_Change_Model_2016)


#check accuracy of model 
predict(Time_For_Change_Model_2016, newdata=Time_For_Change_Data_Only_2016)


```


```{r}
#Exploratory data analysis
Time_For_Change_Correlations <- Time_For_Change_Data %>%
  select(q2gdp, juneapp, inc1, Unemp, Turnout, Petitions, Denied, Prop_Denied, Pop_Growth_Rate, gini, Ratio, Lower40, Upper5)
corrplot(cor(Time_For_Change_Correlations), tl.cex = 0.5)

#Show model trend vs. real stats
plot(Time_For_Change_Data$year, Time_For_Change_Data$incvote, col="red", pch=20)
points(Time_For_Change_Data$year, predict(Time_For_Change_Model_2020), col="blue", pch=20)
plot(Time_For_Change_Data$incvote,predict(Time_For_Change_Model_2020))
```

```{r}
#Cleaning Polling Data
setwd("D:/Git/Election/Data")

Polls_2016 <- read.csv("Polls_2016.csv", stringsAsFactors = FALSE)
Polls_2020 <- read.csv("Polls_2020.csv", stringsAsFactors = FALSE)

Polls_2016 <- Polls_2016 %>%
  select(!X, !johnson, !mcmullin, !question.iteration, !question.text) %>%
  mutate(state = ifelse(state=="--", "National", state))

#######################
# Polling Exploration #
#######################

```